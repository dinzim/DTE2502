{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1232ba9",
      "metadata": {
        "id": "b1232ba9"
      },
      "source": [
        "## Assignment Description\n",
        "Here we will be training a LSTM model using `Adam` and `LDFGS` optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is a common practice to assign seeds to all random number generators in the imported modules this ensures reproducibility across the environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_sine_data(n_samples=100, n_len=1000, width=20):\n",
        "    ''' \n",
        "    n_samples : number of samples\n",
        "    n_len : length of each sample (number of values for each sine wave)\n",
        "    width : width of the wave\n",
        "    '''\n",
        "    N, L, T = n_samples, n_len, width\n",
        "    x = np.empty((N,L), np.float32) # instantiate empty array\n",
        "    x[:] = np.arange(L) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\n",
        "    y = np.sin(x/1.0/T).astype(np.float32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def gen_mixture_sine_data(n_samples=100, n_len=1000, width=20):\n",
        "    ''' \n",
        "    n_samples : number of samples\n",
        "    n_len : length of each sample (number of values for each sine wave)\n",
        "    width : width of the wave\n",
        "    '''\n",
        "    N, L, T = n_samples, n_len, width\n",
        "    x = np.empty((N,L), np.float32) # instantiate empty array\n",
        "    x[:] = np.arange(L) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\n",
        "    y = 0.3*np.sin(x/1.0/T).astype(np.float32) + 0.5* np.sin(x/2.0/T).astype(np.float32) + 0.7* np.cos(x/2.0/T).astype(np.float32) + 1.7* np.cos(x/7.0/T).astype(np.float32)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We generate some sequential data here to train the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data(x, y):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, y)\n",
        "\n",
        "    ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
        "        title='About as simple as it gets, folks')\n",
        "    ax.grid()\n",
        "\n",
        "    #fig.savefig(\"test.png\")\n",
        "    plt.show()\n",
        "\n",
        "X, Y = gen_sine_data() #100 sample sequences each of length 1000\n",
        "X1, Y1 = gen_mixture_sine_data() #100 sample sequences each of length 1000\n",
        "print(X.shape, Y.shape)\n",
        "print(X1.shape, Y1.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_data(X[0], Y[0])\n",
        "plot_data(X[10], Y[10])\n",
        "\n",
        "plot_data(X1[10], Y1[10])\n",
        "plot_data(X1[30], Y1[30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_layers=64):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        # lstm1, lstm2, linear are all layers in the network\n",
        "        self.lstm1 = nn.LSTMCell(1, self.hidden_layers)\n",
        "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
        "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
        "        \n",
        "    def forward(self, y, future_preds=0):\n",
        "        outputs, n_samples = [], y.size(0)\n",
        "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        \n",
        "        for input_t in y.split(1, dim=1):\n",
        "            # N, 1\n",
        "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) # initial hidden and cell states\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
        "            output = self.linear(h_t2) # output from the last FC layer\n",
        "            outputs.append(output)\n",
        "            \n",
        "        for i in range(future_preds):\n",
        "            # this only generates future predictions if we pass in future_preds>0\n",
        "            # mirrors the code above, using last output/prediction as input\n",
        "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs.append(output)\n",
        "        # transform list to tensor    \n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train, validation and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the dataset in train,validation and test sets. For simplicity we do not use a dataloader in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_TEST_SPLIT = 0.3\n",
        "TRAIN_VALID_SPLIT = 0.1\n",
        "\n",
        "DATA_SZ = Y.shape[0]\n",
        "TEST_SZ = int(DATA_SZ*TRAIN_TEST_SPLIT + 0.5)\n",
        "VALID_SZ = int((Y.shape[0] - TEST_SZ)*TRAIN_VALID_SPLIT + 0.5)\n",
        "\n",
        "test_input = torch.from_numpy(Y[:TEST_SZ, :-1]) # (TEST_SZ, 999)\n",
        "test_target = torch.from_numpy(Y[:TEST_SZ, 1:]) # (TEST_SZ, 999)\n",
        "\n",
        "valid_input = torch.from_numpy(Y[TEST_SZ:TEST_SZ+VALID_SZ, :-1]) # (VALID_SZ, 999)\n",
        "valid_target = torch.from_numpy(Y[TEST_SZ:TEST_SZ+VALID_SZ, 1:]) # (VALID_SZ, 999)\n",
        "\n",
        "train_input = torch.from_numpy(Y[TEST_SZ+VALID_SZ:, :-1]) # (rest of data, 999)\n",
        "train_target = torch.from_numpy(Y[TEST_SZ+VALID_SZ:, 1:]) # (rest of data, 999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_epoch(model, optimiser, loss_fn, input, target, future=100, inference=False):\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    losses = []\n",
        "    if inference:\n",
        "        optimiser.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            pred = model(input, future_preds=future)\n",
        "            # use all pred samples, but only until 999 to compare with target\n",
        "            loss = loss_fn(pred[:, :-future], target)\n",
        "            y = pred.detach().numpy()\n",
        "            epoch_loss += loss.item()\n",
        "    else:\n",
        "        y = None\n",
        "        def closure():\n",
        "            optimiser.zero_grad()\n",
        "            out = model(train_input)\n",
        "            loss = loss_fn(out, train_target)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimiser.step(closure)\n",
        "        epoch_loss += losses[0]\n",
        "    return epoch_loss, y\n",
        "    \n",
        "\n",
        "def plot_validation(log_path, idx, train_input, y, future):\n",
        "    # draw figures\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.title(f\"Step {idx+1}\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    n = train_input.shape[1] # 999\n",
        "    def draw(yi, colour):\n",
        "        plt.plot(np.arange(n), yi[:n], colour, linewidth=2.0)\n",
        "        plt.plot(np.arange(n, n+future), yi[n:], colour+\":\", linewidth=2.0)\n",
        "    draw(y[0], 'r')\n",
        "    draw(y[1], 'b')\n",
        "    draw(y[2], 'g')\n",
        "    plt.savefig(os.path.join(log_path,\"predict%d.png\"%idx), dpi=200)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import os \n",
        "\n",
        "def training_loop(n_train_epoch, validation_interval, model, optimiser, criterion, \n",
        "                  train_input, train_target, valid_input, valid_target):\n",
        "    \n",
        "    FUTURE = 1000\n",
        "    training_losses, valid_losses = [], []\n",
        "\n",
        "    best_valid_loss = np.inf\n",
        "    BEST_MODEL_NAME = f'best_lstm_model.pth'\n",
        "    LOG_MODEL = os.path.join(LOG_PATH,'model')\n",
        "    LOG_PLOTS = os.path.join(LOG_PATH,'plots')\n",
        "\n",
        "    if not os.path.exists(LOG_MODEL):\n",
        "        os.makedirs(LOG_MODEL)\n",
        "    if not os.path.exists(LOG_PLOTS):\n",
        "        os.makedirs(LOG_PLOTS)\n",
        "\n",
        "    \n",
        "    for i in range(n_train_epoch):\n",
        "        loss, _ = run_epoch(model, optimiser, criterion, train_input, train_target)\n",
        "        training_losses.append(loss)\n",
        "        if i % validation_interval == 0:\n",
        "            loss, y = run_epoch(model, optimiser, criterion, valid_input, valid_target, \n",
        "                                future=FUTURE, inference=True)\n",
        "            valid_losses.append(loss)\n",
        "            \n",
        "            plot_validation(LOG_PLOTS, i, valid_input, y, FUTURE)\n",
        "\n",
        "            if valid_losses[-1] < best_valid_loss:\n",
        "                print('----- Saving model after validation on epoch {:d} loss {:.6f} < {:.6f} -----'.format(i+1, valid_losses[-1], best_valid_loss))\n",
        "                torch.save(deepcopy(model.state_dict()), os.path.join(LOG_MODEL, BEST_MODEL_NAME))\n",
        "                best_valid_loss = valid_losses[-1]\n",
        "    \n",
        "    return training_losses, valid_losses\n",
        "\n",
        "\n",
        "def plot_losses(train_losses, valid_losses, validation_interval, optim='adam'):\n",
        "    # draw figures\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.title(f\"Train and Validation losses\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    n = len(train_losses)\n",
        "    m = [ i*validation_interval for i in range(len(valid_losses))]\n",
        "    \n",
        "    plt.plot(np.arange(n), np.log10(train_losses), 'b', linewidth=2.0)\n",
        "    plt.plot(m, np.log10(valid_losses), 'g', linewidth=2.0)\n",
        "    plt.savefig(f\"train_valid_losses_{optim}.png\", dpi=200)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate the LSTM model with Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "model1 = LSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser1 = optim.Adam(model1.parameters(), lr=0.0001)\n",
        "current_folder = globals()['_dh'][0]\n",
        "LOG_PATH = os.path.join(current_folder,'logging_adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model with Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_EPOCHS = 100\n",
        "VALID_INTERVAL = 50\n",
        "training_losses, valid_losses = training_loop(N_EPOCHS, VALID_INTERVAL, model1, optimiser1, criterion, \n",
        "                train_input, train_target, valid_input, valid_target)\n",
        "plot_losses(training_losses, valid_losses, VALID_INTERVAL, optim='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate and train the model with LBFGS optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = LSTM()\n",
        "optimiser2 = optim.LBFGS(model2.parameters(), lr=0.08)\n",
        "LOG_PATH = os.path.join(current_folder,'logging_lbfgs')\n",
        "\n",
        "N_EPOCHS = 15\n",
        "VALID_INTERVAL = 5\n",
        "training_losses, valid_losses = training_loop(N_EPOCHS, VALID_INTERVAL, model2, optimiser2, criterion, \n",
        "                  train_input, train_target, valid_input, valid_target)\n",
        "plot_losses(training_losses, valid_losses, VALID_INTERVAL, optim='lbfgs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Task 01`: \n",
        "1. What did you observe about the computation time for single epoch?\n",
        "2. What about the convergence rate of the two optimizers ?\n",
        "3. LGFS is a hessian based gradient descent optimizer. A full analysis of Hessian based approach is beyond the scope of this course. But it is an interesting topic, so read a bit on Hessian based optimization. Now reason about its computational complexity and convergence rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------- Run inference ------------\n",
        "inference_model = LSTM()\n",
        "#Load adam optimized model\n",
        "inference_model.load_state_dict(torch.load(os.path.join(os.path.join(current_folder,'logging_adam'),'model', f'best_lstm_model.pth'), weights_only=True))\n",
        "inference_model.eval()\n",
        "FUTURE = 1000\n",
        "loss, y = run_epoch(inference_model, optimiser2, criterion, test_input, test_target, \n",
        "                                future=FUTURE, inference=True)\n",
        "plot_validation('./', 1, test_input, y, FUTURE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_model = LSTM()\n",
        "#Load lbfgs optimized model\n",
        "inference_model.load_state_dict(torch.load(os.path.join(os.path.join(current_folder,'logging_lbfgs'),'model', f'best_lstm_model.pth'), weights_only=True))\n",
        "inference_model.eval()\n",
        "FUTURE = 1000\n",
        "loss, y = run_epoch(inference_model, optimiser2, criterion, test_input, test_target, \n",
        "                                future=FUTURE, inference=True)\n",
        "plot_validation('./', 2, test_input, y, FUTURE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Task 02`: Comment on the performance of the to models during the training and inference\n",
        "\n",
        "Note: You will find training plots per epoch in the `logging_adam/plots` and `logging_lbfgs/plots` directories"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "interpreter": {
      "hash": "f5714fd0a3546024bcb03838307f86f57bdfb736019eafd13d03debd06928b59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
