{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1232ba9",
      "metadata": {
        "id": "b1232ba9"
      },
      "source": [
        "## Assignment Description\n",
        "We have discussed CNN model in the lectures to classify the given handwritten digit into one of 10 integer classes (0-9). Download the `CIFAR` dataset using `from torchvision.datasets import CIFAR10`. The dataset contains images of ten classes that your CNN model implemented in PyTorch is going to classify `classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KtW6_X1vF8GY",
      "metadata": {
        "id": "KtW6_X1vF8GY"
      },
      "source": [
        "In this assignment you can use the code from the model explained during the [lecture](https://github.com/krayyalasomayajula/DTE2502/blob/main/2025/mod04/pytorch.ipynb) or other resources. However, you are expected to make the following changes so the your code works. Use your `uitnn` environment:\n",
        "\n",
        "`Task 01` Data loader with the following capabilities:\n",
        "1. Perform data preprocessing suing Dataloader to read read transform data.\n",
        "Data transformation can be done as follows:\n",
        "\n",
        "`transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalizing for RGB channels\n",
        "])\n",
        "`\n",
        "\n",
        "2. Split the dataset into Train, Validation and Test sets. Create their corresponding Dataloaders.\n",
        "3. As always try to visualize a few images from the datasets to familiarize with the dataset. Note: Visualizing function code will be implemented by you, that will be using the data through dataloader and not part of the Dataloader class in pytorch.\n",
        "\n",
        "`Task 02` Model experiments:\n",
        "1. Develop two deep learning models one with a) one CNN block only another with b) with 2 CNN block to solve this classiﬁcation task. (Remember a CNN block = CNN layer + ReLU activation + Maxpool layer)\n",
        "2. Train and evaluate your models to establish a baseline of performance using the `Adam` optimizer.\n",
        "4. Tweak the network's hyper-parameters to make each model perform optimally. Hyper-parameters to think about are learning-rate= `1e-4` to `1e-1`, batch-size = 8-64, convolution kernel-size for = 3,5,7. You may use the optuna package that automates and finds hyperparameter in fast ans optimal way. Install: `pip install optuna`, an exmaple on optuna package usage is given [here](https://medium.com/@boukamchahamdi/fine-tuning-a-resnet18-model-with-optuna-hyperparameter-optimization-2e3eab0bcca7)\n",
        "5. Repeat steps 1-3 for both models and pick the model which performs best with tuned hyper-params on test data (based on class prediction accuracy).\n",
        "\n",
        "`Task 03` Optimization: For the best model (1 CNN or 2CNN block model with optimized hyper-parameters) perform the following steps.\n",
        "1. Diagnose the model for each case (overﬁt/underﬁt), and explain the model's behavior by using the learning curves (learning curves are plots of training and validation losses for each epoch). Remember overfit: very low training loss but validation loss is high. underfit: good training training loss but validation loss has lowered but plateued after some epochs.\n",
        "2. Benchmark your model's performance using SGD, Momentum, and Adam optimizers.\n",
        "\n",
        "`Task 04` Discussion:\n",
        "1. Discuss if adding the second CNN block has improved the model performance.\n",
        "2. Report the effects of optimers during traing and their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "interpreter": {
      "hash": "f5714fd0a3546024bcb03838307f86f57bdfb736019eafd13d03debd06928b59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
